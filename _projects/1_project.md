---
layout: page
title: "Taga-VLM"
description: "Topology-Aware Global Action Reasoning for Vision-Language Navigation"
img: assets/img/12.jpg
importance: 1
category: on-going
related_publications: false
---

We propose a novel Vision-Language Navigation (VLN) agent powered by Large Vision-Language Models (LVLMs). By leveraging the strong reasoning and multimodal alignment capabilities of LVLMs, the agent can autonomously navigate through previously unseen indoor environments by interpreting natural language instructions and analyzing panoramic visual observations. A key advantage of this framework is its versatility: it demonstrates robust navigation performance and generalization abilities in both discrete (graph-based) and continuous (Euclidean) environments, effectively bridging the gap between high-level semantic understanding and low-level action execution. More details will coming soon.

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/video/D7N2EKCX4Sj-7337-SPL0.93.mp4" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/video/D7N2EKCX4Sj-7808-SPL0.77.mp4" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Visualization of the proposed VLN approch.
</div>

---

## Related Projects

- Generalizable Vision-Language Navigation across Diverse Scenarios.
- Vision-Language Navigation with Look-Ahead Exploration.
- Speaker-Follower Models for Vision-Language Navigation.
